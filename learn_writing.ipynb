{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d0c51a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\pythonAI\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='models/gpt2-chinese-cluecorpussmall', vocab_size=21128, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "{'input_ids': [[101, 3617, 1139, 3313, 1139, 1045, 6793, 6809, 117, 1283, 2255, 674, 2255, 1963, 4125, 1355, 119, 7557, 5640, 6624, 1403, 1921, 677, 3341, 117, 6852, 1316, 3655, 3215, 6628, 1316, 3299, 119, 102], [101, 4007, 4680, 3736, 2255, 1724, 3307, 2406, 117, 4635, 756, 7770, 1318, 2322, 4170, 3119, 119, 3189, 1726, 4896, 2512, 4959, 4541, 3312, 117, 7599, 6853, 4351, 1898, 1057, 2207, 3517, 119, 6823, 2273, 849, 2242, 3566, 4819, 5862, 117, 3171, 2359, 1963, 1383, 2779, 704, 3837, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 设置本地模型路径\n",
    "local_model_path = \"models/gpt2-chinese-cluecorpussmall\"  # 请替换为你的本地路径\n",
    "\n",
    "# 加载本地的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# 编码试算\n",
    "encoded = tokenizer.batch_encode_plus([\n",
    "    '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.',\n",
    "    '满目江山四望幽,白云高卷嶂烟收.日回禽影穿疏木,风递猿声入小楼.远岫似屏横碧落,断帆如叶截中流.'\n",
    "])\n",
    "\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc8dfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304752, '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 使用utf-8编码打开文件\n",
    "        with open('chinese_poems.txt', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [i.strip() for i in lines]\n",
    "\n",
    "        self.lines = lines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.lines[i]\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "len(dataset), dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750a592f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(839587, '云髻高梳鬓不分，扫除虚室事元君。新糊白纸屏风上，尽画蓬莱五色云。')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#更多数据数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        data = []\n",
    "        for i in os.listdir('more_datas'):\n",
    "            if i == '.ipynb_checkpoints':\n",
    "                continue\n",
    "            data.append(pd.read_csv('more_datas/%s' % i))\n",
    "\n",
    "        data = pd.concat(data).reset_index()\n",
    "\n",
    "        data = data['内容']\n",
    "\n",
    "        data = data.str.strip()\n",
    "\n",
    "        #移除一些标点符号\n",
    "        data = data.str.replace('[《》“”「」]', '', regex=True)\n",
    "\n",
    "        #正则过滤\n",
    "        select = data.str.match('^[\\w，。？、！：；]+$', na=False)\n",
    "        data = data[select]\n",
    "\n",
    "        #标点符号合并\n",
    "        data = data.str.replace('[？！；]', '。', regex=True)\n",
    "        data = data.str.replace('[、：]', '，', regex=True)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data.iloc[i]\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405095b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 82])\n",
      "token_type_ids torch.Size([8, 82])\n",
      "attention_mask torch.Size([8, 82])\n",
      "labels torch.Size([8, 82])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104948"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    data = tokenizer.batch_encode_plus(data,\n",
    "                                       padding=True,\n",
    "                                       truncation=True,\n",
    "                                       max_length=512,\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53f3c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count (in 10k): 10206.8736\n",
      "torch.Size([1, 10, 21128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 设置本地模型路径\n",
    "local_model_path = \"models/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "# 加载本地预训练模型和tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# 统计模型参数数量\n",
    "param_count = sum(i.numel() for i in model.parameters()) / 10000\n",
    "print(f'Model parameter count (in 10k): {param_count}')\n",
    "\n",
    "# 准备输入数据\n",
    "text = \"今天的天气真好！\"\n",
    "data = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 在不计算梯度的情况下进行推理\n",
    "with torch.no_grad():\n",
    "    out = model(**data)\n",
    "\n",
    "# 输出logits的形状\n",
    "print(out.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd9b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS] 秋 季 秋 季 男 ， 女 是 一 季 节 。 有 没 有 女 性 ， 如 何 确 定 秋 。\n",
      "1 [CLS] 秋 冬 季 节 如 ， 那 些 美 女 们 。 一 直 都 是 不 ， 秋 冬 季 节 如 。\n",
      "2 [CLS] 秋 季 如 何 去 ， 去 哪 里 ？ 作 。 作 为 南 方 人 ， 我 觉 得 秋 季 。\n"
     ]
    }
   ],
   "source": [
    "def generate(text, row, col):\n",
    "\n",
    "    def generate_loop(data):\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "\n",
    "        #取最后一个字\n",
    "        #[5, b, 50257]\n",
    "        out = out['logits']\n",
    "        #[5, 50257]\n",
    "        out = out[:, -1]\n",
    "\n",
    "        #第50大的值,以此为分界线,小于该值的全部赋值为负无穷\n",
    "        #[5, 50257] -> [5, 50]\n",
    "        topk_value = torch.topk(out, 50).values\n",
    "        #[5, 50] -> [5] -> [5, 1]\n",
    "        topk_value = topk_value[:, -1].unsqueeze(dim=1)\n",
    "\n",
    "        #赋值\n",
    "        #[5, 50257]\n",
    "        out = out.masked_fill(out < topk_value, -float('inf'))\n",
    "\n",
    "        #不允许写特殊符号\n",
    "        out[:, tokenizer.sep_token_id] = -float('inf')\n",
    "        out[:, tokenizer.unk_token_id] = -float('inf')\n",
    "        out[:, tokenizer.pad_token_id] = -float('inf')\n",
    "        for i in '，。':\n",
    "            out[:, tokenizer.get_vocab()[i]] = -float('inf')\n",
    "\n",
    "        #根据概率采样,无放回,所以不可能重复\n",
    "        #[5, 50257] -> [5, 1]\n",
    "        out = out.softmax(dim=1)\n",
    "        out = out.multinomial(num_samples=1)\n",
    "\n",
    "        #强制添加标点符号\n",
    "        c = data['input_ids'].shape[1] / (col + 1)\n",
    "        if c % 1 == 0:\n",
    "            if c % 2 == 0:\n",
    "                out[:, 0] = tokenizer.get_vocab()['。']\n",
    "            else:\n",
    "                out[:, 0] = tokenizer.get_vocab()['，']\n",
    "\n",
    "        data['input_ids'] = torch.cat([data['input_ids'], out], dim=1)\n",
    "        data['attention_mask'] = torch.ones_like(data['input_ids'])\n",
    "        data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n",
    "        data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "        if data['input_ids'].shape[1] >= row * col + row + 1:\n",
    "            return data\n",
    "\n",
    "        return generate_loop(data)\n",
    "\n",
    "    #重复3遍\n",
    "    data = tokenizer.batch_encode_plus([text] * 3, return_tensors='pt')\n",
    "    data['input_ids'] = data['input_ids'][:, :-1]\n",
    "    data['attention_mask'] = torch.ones_like(data['input_ids'])\n",
    "    data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "\n",
    "    data = generate_loop(data)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(i, tokenizer.decode(data['input_ids'][i]))\n",
    "\n",
    "\n",
    "generate('秋', row=4, col=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0e6ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\pythonAI\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                                           | 1/104948 [00:03<105:12:46,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.860363960266113 4.99995235735793e-05 0.1098546042003231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 5/104948 [00:17<99:29:28,  3.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 调用训练函数\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\python3\\pythonAI\\venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python3\\pythonAI\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python3\\pythonAI\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "import torch\n",
    "\n",
    "# 1. 设置本地模型路径\n",
    "local_model_path = \"models/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "# 2. 加载 GPT2LMHeadModel 和 tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(local_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# 训练数据 loader\n",
    "# loader = ...\n",
    "\n",
    "# 训练函数\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # 使用 tqdm 包装数据加载器，显示进度条\n",
    "    for i, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "        \n",
    "        # 前向传播和损失计算\n",
    "        out = model(**data)\n",
    "        loss = out.loss\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            labels = data['labels'][:, 1:]\n",
    "            out = out.logits.argmax(dim=2)[:, :-1]\n",
    "\n",
    "            select = labels != 0\n",
    "            labels = labels[select]\n",
    "            out = out[select]\n",
    "            del select\n",
    "\n",
    "            accuracy = (labels == out).sum().item() / labels.numel()\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            # 输出当前训练状态\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "    print(\"Training finished, saving model...\")\n",
    "    torch.save(model, 'save.model')\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "# 调用训练函数\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('save.model')\n",
    "\n",
    "generate('秋', row=4, col=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
